{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Problem with friend's method:\n",
    "\n",
    "``MemoryError`` typically happens when the code is loading too much data and it exceeds the memory allocated to python program. Although the computer has RAM of 8GB and the file is 4GB, memory error can still happen, because \n",
    "\n",
    "1. The program is not able to use all the 8GB RAM. Some other background activities on the computer may takes some RAM. For example, the web browser, the Microsoft Word or some other applications takes some RAM and makes the RAM available for python smaller. For detailed RAM activities, my friend can open the \"Activity Monitor\" and see the allocation of RAM to different programs.\n",
    "2. Python is less \"memory-efficient\" than other programming languages. Python use Python Runtime Environment (PRE) to manage the memory that will not be used in the future and free the memory. In contrast, other languages such as C++ requires the programmer to free the memory themselves. PRE makes python uses more RAM [1]. Besides, when storing the same int number, python will use more space to store it. All these reasons makes it difficult for python to directly load the 4GB data. \n",
    "\n",
    "In one sentence, the file is too large, and thus my friend cannot simply load all these data into python. \n",
    "\n",
    "*Reference*:\n",
    "\n",
    "[1] Zehra F, Javed M, Khan D, et al. Comparative analysis of C++ and Python in terms of memory and time[J]. 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "To solve the issue while still storing all these data in memory, following methods may be helpful:\n",
    "\n",
    "1. Ask the friend to purchase a larger RAM in the computer store nearby. Change to a 16GB RAM, for example, may be helpful.\n",
    "2. It 1 is not applicable, my friend can split the data into several parts and process them separately. For example, my friend can split the data into 100 files, each contains 5 million people and calculate their average. And finally calculate the average of average. \n",
    "3. Some virtual memory techniques may be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "There is also a strategy for calculating the average that would not require storing all the data in memory. That is to change the code so that there does not exist such a giant variable. We add the newly read weight number as a sum instead of appending it into a list. Then divide the sum of weights by total number of people to calculate the average weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('weights.txt') as f:\n",
    "    weights = 0\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        weights += float(line)\n",
    "        count += 1\n",
    "print(\"average =\", weights / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
